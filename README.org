Code for reproducing SOAK paper results for NSCH data

[[https://docs.google.com/presentation/d/1EVu7YMCO4tvK3RhkUp1dfFABspn410X9bz2DBznsO-g/edit?usp=sharing][Google slides summarizing results]].

* Context

National Survey of Public Health (NSCH) data from 2019 and 2020 were
used as one of the 20 data sets in the paper, [[https://onlinelibrary.wiley.com/doi/10.1002/sam.70055][Bodine, Thibault,
Arellano, Shenkin, Lindly, Hocking. SOAK: Same/Other/All K-fold
cross-validation for estimating similarity of patterns in data
subsets. Statistical Analysis and Data Mining 19(1), 2026]].

The raw data were converted to a form suitable for machine learning:
CSV with one row per person who responded to the survey, one column
per categorical response variable.
These data are available on [[https://zenodo.org/records/18273949][zenodo]].
The autism diagnosis column was
used as the output/response to predict, and the other columns were
used as the inputs to the prediction function.
Figure 4 and 5 show results computed using these data.

* Reproducing machine learning prediction error/accuracy figures

The original result CSV files used to compute Figures 4 and 5, and
reproducibility instructions, are available in the [[https://github.com/tdhock/cv-same-other-paper?tab=readme-ov-file#reproducibility][GitHub repository associated with the paper]].

This repo provides code for three different ways of reproducing these results.

** Step 0 clone this repo

On a cluster system, you typically want this in your project space.

#+begin_src shell-script
  cd /projects/thocking
  git clone https://github.com/NAU-ASD3/reproduce-soak-nsch
  cd reproduce-soak-nsch
#+end_src

On your laptop, typically under your home directory.

#+begin_src shell-script
  cd
  git clone https://github.com/NAU-ASD3/reproduce-soak-nsch
  cd reproduce-soak-nsch
#+end_src

** Step 1 create scratch link

All scripts

- first save data and result files under =scratch/=
- and then copy result files under =results/=

So on a cluster system, you should make sure scratch is a link to a directory under your scratch space.
For example, you can make a symlink by running the following commands

#+begin_src shell-script
  cd /projects/thocking/reproduce-soak-nsch
  mkdir /scratch/thocking/scratch-soak-nsch
  ln -s /scratch/thocking/scratch-soak-nsch scratch
#+end_src

On your laptop, you donâ€™t need to create a scratch symlink (a scratch sub-directory will be created for you).

** Step 2 launch calculations

There are three options implemented.
Start R in this directory, and run the code in one of these files.

*** Local and MPI

- Option 1: [[file:NSCH_local.R]] is the simplest method (and slowest): it runs using your local computer (possibly in parallel).
- Option 2: [[file:NSCH_mpi.R]] requires a cluster with SLURM and MPI.

These methods both use [[file:NSCH_proj.R]] to setup the mlr3resampling project.

*** batchtools

- Option 3: [[file:NSCH_batchtools.R]] uses batchtools, so works with other kinds of clusters (not just SLURM).

All options use [[file:NSCH.R]] to setup the mlr3 Task, Learner, and SOAK Resampling instance.

** Step 3 Wait and copy CSV files to results sub-directory

First, wait for the calculations to finish (5 minutes to 20 hours depending on your computer).

- For options 1 and 2, the main result file is =scratch/proj/results.csv=
- For option 3, the results are saved under =scratch/registry=

You can use [[file:results_copy.R]] to copy CSV files to a sub-directory of results.
For example, the results I computed are in [[file:results/2026-02-24/]]

** Step 4 Result figures

- [[file:results/2026-02-24/figure-compare-paper.R]] makes several figures that compare the original and reproduced results.
- [[file:results/2026-02-24/figure-compare-computation.R]] makes a figure to compare the results from the different computational methods.
- [[file:results/2026-02-24/figure-timings.R]] makes a figure to compare the computation time of the different computational methods.

* Reproducing data generation

[[file:data_Classif_NSCH_autism.R]] has code for downloading Stata files
from NSCH, and converting into a format for machine learning.

When this code was originally run in Jan 2024, the files provided by
NSCH were named like =nsch_2019_topical.dta= and gave the result below.
#+begin_src R
    year data.type  nrow  ncol questions  %Autism %rowsNA  %colsNA
1:  2019       raw 29433   443       443 2.961548     100 90.06772
2:  2019 processed 18202   377       187 2.999670       0  0.00000
3:  2020       raw 42777   443       443 2.975757     100 90.06772
4:  2020 processed 27808   373       185 3.081847       0  0.00000
#+end_src

Re-running the code on the new files named like
=nsch_2019e_topical.dta= (e added after year) gave the result below.
#+begin_src R
    year data.type  nrow  ncol questions  %Autism %rowsNA  %colsNA
1:  2019       raw 29433   448       448 2.961548     100 89.73214
2:  2019 processed 18187   380       190 2.996646       0  0.00000
3:  2020       raw 42777   447       447 2.975757     100 89.70917
4:  2020 processed 27776   376       188 3.085397       0  0.00000
#+end_src

We see the raw number of rows is consistent, but there are more
columns in the raw data.
The numbers after processing are not equal, but they are reasonably similar.

